{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf1ycO8PbdCSr/SPdkW5V0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vvslaxman/core-model-gpt/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7N0UcMMlXNz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "VQh-LjmdlgPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0843e41-5aff-4824-b5ef-59025bd5940f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bfadc0dd550>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text data\n",
        "with open(\"/tiny-shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "cjiepbVGEtlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "CctFi3LUFDqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rBwvJ6clFqqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "hhrl15QfLIuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[nn.TransformerEncoderLayer(n_embd, n_head, n_embd * 4) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(idx.size(1), device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_sequence = model.generate(context, max_new_tokens=2000)[0].tolist()\n",
        "print(decode(generated_sequence))"
      ],
      "metadata": {
        "id": "8G4meNifLMfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d5495f-052b-408d-b4d8-024b5bb2d9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3923, val loss 4.3965\n",
            "step 100: train loss 2.6910, val loss 2.7114\n",
            "step 200: train loss 2.5723, val loss 2.5745\n",
            "step 300: train loss 2.5399, val loss 2.5482\n",
            "step 400: train loss 2.5226, val loss 2.5347\n",
            "step 500: train loss 2.5080, val loss 2.5172\n",
            "step 600: train loss 2.5151, val loss 2.5175\n",
            "step 700: train loss 2.5056, val loss 2.5160\n",
            "step 800: train loss 2.4997, val loss 2.5122\n",
            "step 900: train loss 2.4990, val loss 2.5143\n",
            "step 1000: train loss 2.4998, val loss 2.5195\n",
            "step 1100: train loss 2.4914, val loss 2.5115\n",
            "step 1200: train loss 2.4883, val loss 2.5039\n",
            "step 1300: train loss 2.4865, val loss 2.5012\n",
            "step 1400: train loss 2.4830, val loss 2.4964\n",
            "step 1500: train loss 2.4857, val loss 2.4950\n",
            "step 1600: train loss 2.4875, val loss 2.5091\n",
            "step 1700: train loss 2.4794, val loss 2.5055\n",
            "step 1800: train loss 2.4814, val loss 2.5049\n",
            "step 1900: train loss 2.4828, val loss 2.4996\n",
            "step 2000: train loss 2.4803, val loss 2.5032\n",
            "step 2100: train loss 2.4822, val loss 2.4994\n",
            "step 2200: train loss 2.4808, val loss 2.4891\n",
            "step 2300: train loss 2.4778, val loss 2.5031\n",
            "step 2400: train loss 2.4818, val loss 2.4913\n",
            "step 2500: train loss 2.4830, val loss 2.4972\n",
            "step 2600: train loss 2.4788, val loss 2.4890\n",
            "step 2700: train loss 2.4766, val loss 2.5049\n",
            "step 2800: train loss 2.4760, val loss 2.5035\n",
            "step 2900: train loss 2.4820, val loss 2.4925\n",
            "step 3000: train loss 2.4779, val loss 2.5011\n",
            "step 3100: train loss 2.4754, val loss 2.4913\n",
            "step 3200: train loss 2.4841, val loss 2.4931\n",
            "step 3300: train loss 2.4720, val loss 2.4946\n",
            "step 3400: train loss 2.4704, val loss 2.4981\n",
            "step 3500: train loss 2.4724, val loss 2.5026\n",
            "step 3600: train loss 2.4733, val loss 2.4893\n",
            "step 3700: train loss 2.4747, val loss 2.5044\n",
            "step 3800: train loss 2.4733, val loss 2.4903\n",
            "step 3900: train loss 2.4738, val loss 2.4944\n",
            "step 4000: train loss 2.4725, val loss 2.5048\n",
            "step 4100: train loss 2.4668, val loss 2.4892\n",
            "step 4200: train loss 2.4698, val loss 2.4825\n",
            "step 4300: train loss 2.4722, val loss 2.4995\n",
            "step 4400: train loss 2.4739, val loss 2.5037\n",
            "step 4500: train loss 2.4676, val loss 2.4930\n",
            "step 4600: train loss 2.4742, val loss 2.4980\n",
            "step 4700: train loss 2.4749, val loss 2.5016\n",
            "step 4800: train loss 2.4756, val loss 2.5009\n",
            "step 4900: train loss 2.4714, val loss 2.4917\n",
            "step 5000: train loss 2.4763, val loss 2.4987\n",
            "step 5100: train loss 2.4755, val loss 2.5044\n",
            "step 5200: train loss 2.4769, val loss 2.4984\n",
            "step 5300: train loss 2.4749, val loss 2.4908\n",
            "step 5400: train loss 2.4705, val loss 2.4829\n",
            "step 5500: train loss 2.4703, val loss 2.4921\n",
            "step 5600: train loss 2.4712, val loss 2.5034\n",
            "step 5700: train loss 2.4622, val loss 2.4921\n",
            "step 5800: train loss 2.4679, val loss 2.4958\n",
            "step 5900: train loss 2.4697, val loss 2.5025\n",
            "step 6000: train loss 2.4743, val loss 2.4948\n",
            "step 6100: train loss 2.4759, val loss 2.4927\n",
            "step 6200: train loss 2.4687, val loss 2.4875\n",
            "step 6300: train loss 2.4706, val loss 2.4991\n",
            "step 6400: train loss 2.4631, val loss 2.4824\n",
            "step 6500: train loss 2.4687, val loss 2.4928\n",
            "step 6600: train loss 2.4750, val loss 2.4959\n",
            "step 6700: train loss 2.4746, val loss 2.4965\n",
            "step 6800: train loss 2.4699, val loss 2.4938\n",
            "step 6900: train loss 2.4617, val loss 2.5002\n",
            "step 7000: train loss 2.4665, val loss 2.4921\n",
            "step 7100: train loss 2.4683, val loss 2.4911\n",
            "step 7200: train loss 2.4658, val loss 2.4996\n",
            "step 7300: train loss 2.4752, val loss 2.4853\n",
            "step 7400: train loss 2.4696, val loss 2.4858\n",
            "step 7500: train loss 2.4776, val loss 2.4860\n",
            "step 7600: train loss 2.4661, val loss 2.4921\n",
            "step 7700: train loss 2.4669, val loss 2.4847\n",
            "step 7800: train loss 2.4692, val loss 2.4836\n",
            "step 7900: train loss 2.4692, val loss 2.4893\n",
            "step 8000: train loss 2.4715, val loss 2.4989\n",
            "step 8100: train loss 2.4653, val loss 2.4964\n",
            "step 8200: train loss 2.4712, val loss 2.4848\n",
            "step 8300: train loss 2.4692, val loss 2.4947\n",
            "step 8400: train loss 2.4631, val loss 2.4901\n",
            "step 8500: train loss 2.4704, val loss 2.5016\n",
            "step 8600: train loss 2.4671, val loss 2.4962\n",
            "step 8700: train loss 2.4655, val loss 2.4915\n",
            "step 8800: train loss 2.4679, val loss 2.4932\n",
            "step 8900: train loss 2.4686, val loss 2.4917\n",
            "step 9000: train loss 2.4680, val loss 2.4832\n",
            "step 9100: train loss 2.4637, val loss 2.4916\n",
            "step 9200: train loss 2.4686, val loss 2.4841\n",
            "step 9300: train loss 2.4713, val loss 2.5021\n",
            "step 9400: train loss 2.4711, val loss 2.4929\n",
            "step 9500: train loss 2.4740, val loss 2.4945\n",
            "step 9600: train loss 2.4721, val loss 2.4932\n",
            "step 9700: train loss 2.4622, val loss 2.4922\n",
            "step 9800: train loss 2.4673, val loss 2.4874\n",
            "step 9900: train loss 2.4678, val loss 2.4920\n",
            "step 9999: train loss 2.4701, val loss 2.4850\n",
            "\n",
            "Y torer hin, je dlle s t ill chanelin m nehat ieyed CUK limarcowelaresparat d belo me velloth paspavime POFod he busthil:\n",
            "Pounateslerkl pe ditho walind brak t, nfod sqou thasecertirarthan'sun awhabulfuinainerthive t, g yousect t' ind, ind IO ny meloforst harape, cesss f thon'daite, rd llle ounepedeeee hindy. t g ouchothefe ckdery arde l wwou, o m as Fod, he d thenidorthye cornd geseret tien u fe; a tho'eershane fomshalt ld t s,\n",
            "TESetatofisig tevetheaswindeeathepenk br ve tongais sorde shaneeman! el phaugl im, y;\n",
            "ANThao twhard n: h de:\n",
            "Hendar.ance s prer cartais toma t? than l Corvapr chey th y tlered\n",
            "TI:\n",
            "Anc n O:\n",
            "'d mo nghem thoundd cr m id f:\n",
            "Ply aN l y cerdver d ceghat he ds IN hof s dinf o ll\n",
            "Led mmow ns; s wisse.\n",
            ". Pare\n",
            "Bue t t tten,-\n",
            "LOLy\n",
            "A I: cer dut t rthim wanfomby;\n",
            "War pe sthay tondshe ndeant wokecher'ld?\n",
            "We hofo l y ms f:\n",
            "Mp, r amny; ovat tom d e, hecoulldsbgimeashy frin hizepes'. nur t y IO:\n",
            "Yousheut:\n",
            "BESomyofI surins n venchagr hacecr,\n",
            "Thistouioteane'de, cat, Prt buald edatars d it ayo, yom,\n",
            "Wol' l shin sthe or higs, B far; MIndu s can, gn atoupre my fovearom? br w\n",
            "NAsthed ply, qoasuis hen ondomons y y frouthtis l us:\n",
            "USomeds gortht s\n",
            "ASarld.\n",
            "Woepofacal lathakn; mm\n",
            "Gournfedovilothele tond; l? ens peve cetomma idy I t at t'e y tre htou th. weal t thaposesthathalloud s, keathand meact t? toungh courdomus t w'sh houe, ndsole his st'stheprcedgreas\n",
            "Matothe; gofAL note owall m: wig't arso c:\n",
            "Bo knch strsad nseer wne\n",
            "\n",
            "\n",
            "\n",
            "PO, fr hiot w Whaine gercanoulvert. geido sple min, mel chelgs inintob thy he ld bu bitayonchanslanco de he A thell d w t s hond'd Ine nd Pathathene:\n",
            "Teathef otho yeanou ced, hor ICll e BUsar in ptest a, nl mat whas. chatthisin lan h arnds m k tof s\n",
            "Wore hilor CESustiepollllanff begy ter windde per ish:\n",
            "D:\n",
            "Tou owe wis tthean. adoue p win, quthou bustharthand t se\n",
            "Bunor w bes RDY cel beng h ar Whout wases!\n",
            "CE thowisos; n al hag I; s\n",
            "Thucantisof me pule sthintok Pavet\n",
            "Betour, setisthat.\n",
            "A dencavend, fe migrd ilr ray d dofatheth gd tha arket, tithe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "vocab_size = 256  # Adjust according to your vocabulary size\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "hidden_dim = 256\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Load text data\n",
        "with open(\"/tiny-shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + self.dropout(self.norm1(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + self.dropout(self.norm2(ffn_output))\n",
        "        return x\n",
        "\n",
        "class TransformerLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, hidden_dim=256, num_layers=6, dropout=0.1):\n",
        "        super(TransformerLanguageModel, self).__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, embed_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, hidden_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(idx.size(1), device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = TransformerLanguageModel(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_sequence = model.generate(context, max_new_tokens=2000)[0].tolist()\n",
        "print(decode(generated_sequence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oBAJEwTL6ee",
        "outputId": "9dc25f4f-c515-4ed1-8747-18dcddb93db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 6.2801, val loss 6.2647\n",
            "step 100: train loss 2.6968, val loss 2.7033\n",
            "step 200: train loss 2.5925, val loss 2.5925\n",
            "step 300: train loss 2.5749, val loss 2.5824\n",
            "step 400: train loss 2.5455, val loss 2.5630\n",
            "step 500: train loss 2.5366, val loss 2.5415\n",
            "step 600: train loss 2.5302, val loss 2.5540\n",
            "step 700: train loss 2.5147, val loss 2.5209\n",
            "step 800: train loss 2.5116, val loss 2.5277\n",
            "step 900: train loss 2.5083, val loss 2.5126\n",
            "step 1000: train loss 2.5153, val loss 2.5221\n",
            "step 1100: train loss 2.5035, val loss 2.5495\n",
            "step 1200: train loss 2.4935, val loss 2.5138\n",
            "step 1300: train loss 2.4898, val loss 2.5141\n",
            "step 1400: train loss 2.4993, val loss 2.5139\n",
            "step 1500: train loss 2.4987, val loss 2.5142\n",
            "step 1600: train loss 2.5011, val loss 2.5335\n",
            "step 1700: train loss 2.4897, val loss 2.5113\n",
            "step 1800: train loss 2.4925, val loss 2.5128\n",
            "step 1900: train loss 2.4940, val loss 2.4963\n",
            "step 2000: train loss 2.4911, val loss 2.5166\n",
            "step 2100: train loss 2.4988, val loss 2.5261\n",
            "step 2200: train loss 2.4907, val loss 2.5149\n",
            "step 2300: train loss 2.4896, val loss 2.5210\n",
            "step 2400: train loss 2.4859, val loss 2.5099\n",
            "step 2500: train loss 2.4880, val loss 2.5117\n",
            "step 2600: train loss 2.4894, val loss 2.5160\n",
            "step 2700: train loss 2.4991, val loss 2.5142\n",
            "step 2800: train loss 2.4969, val loss 2.5166\n",
            "step 2900: train loss 2.4793, val loss 2.5082\n",
            "step 3000: train loss 2.4862, val loss 2.5201\n",
            "step 3100: train loss 2.4855, val loss 2.5137\n",
            "step 3200: train loss 2.4827, val loss 2.4998\n",
            "step 3300: train loss 2.4867, val loss 2.5040\n",
            "step 3400: train loss 2.4843, val loss 2.5147\n",
            "step 3500: train loss 2.4872, val loss 2.5002\n",
            "step 3600: train loss 2.4910, val loss 2.5094\n",
            "step 3700: train loss 2.4865, val loss 2.5114\n",
            "step 3800: train loss 2.4832, val loss 2.5084\n",
            "step 3900: train loss 2.4860, val loss 2.5089\n",
            "step 4000: train loss 2.4819, val loss 2.5070\n",
            "step 4100: train loss 2.4807, val loss 2.5151\n",
            "step 4200: train loss 2.4789, val loss 2.5075\n",
            "step 4300: train loss 2.4800, val loss 2.4930\n",
            "step 4400: train loss 2.4859, val loss 2.5105\n",
            "step 4500: train loss 2.4870, val loss 2.5010\n",
            "step 4600: train loss 2.4814, val loss 2.5056\n",
            "step 4700: train loss 2.4770, val loss 2.5064\n",
            "step 4800: train loss 2.4766, val loss 2.4995\n",
            "step 4900: train loss 2.4799, val loss 2.5058\n",
            "step 5000: train loss 2.4758, val loss 2.4973\n",
            "step 5100: train loss 2.4813, val loss 2.5081\n",
            "step 5200: train loss 2.4730, val loss 2.5175\n",
            "step 5300: train loss 2.4748, val loss 2.5010\n",
            "step 5400: train loss 2.4837, val loss 2.5064\n",
            "step 5500: train loss 2.4742, val loss 2.5051\n",
            "step 5600: train loss 2.4765, val loss 2.5099\n",
            "step 5700: train loss 2.4829, val loss 2.5015\n",
            "step 5800: train loss 2.4812, val loss 2.5006\n",
            "step 5900: train loss 2.4806, val loss 2.5092\n",
            "step 6000: train loss 2.4798, val loss 2.5018\n",
            "step 6100: train loss 2.4800, val loss 2.5080\n",
            "step 6200: train loss 2.4822, val loss 2.5063\n",
            "step 6300: train loss 2.4775, val loss 2.5120\n",
            "step 6400: train loss 2.4777, val loss 2.5016\n",
            "step 6500: train loss 2.4787, val loss 2.4940\n",
            "step 6600: train loss 2.4757, val loss 2.5058\n",
            "step 6700: train loss 2.4766, val loss 2.5008\n",
            "step 6800: train loss 2.4847, val loss 2.5062\n",
            "step 6900: train loss 2.4764, val loss 2.5045\n",
            "step 7000: train loss 2.4822, val loss 2.5104\n",
            "step 7100: train loss 2.4799, val loss 2.4984\n",
            "step 7200: train loss 2.4791, val loss 2.4980\n",
            "step 7300: train loss 2.4785, val loss 2.5039\n",
            "step 7400: train loss 2.4740, val loss 2.4962\n",
            "step 7500: train loss 2.4735, val loss 2.5088\n",
            "step 7600: train loss 2.4726, val loss 2.4919\n",
            "step 7700: train loss 2.4798, val loss 2.5040\n",
            "step 7800: train loss 2.4823, val loss 2.5057\n",
            "step 7900: train loss 2.4778, val loss 2.5075\n",
            "step 8000: train loss 2.4731, val loss 2.5062\n",
            "step 8100: train loss 2.4760, val loss 2.5057\n",
            "step 8200: train loss 2.4778, val loss 2.4912\n",
            "step 8300: train loss 2.4754, val loss 2.5058\n",
            "step 8400: train loss 2.4763, val loss 2.5060\n",
            "step 8500: train loss 2.4712, val loss 2.4985\n",
            "step 8600: train loss 2.4746, val loss 2.5079\n",
            "step 8700: train loss 2.4742, val loss 2.4983\n",
            "step 8800: train loss 2.4710, val loss 2.5070\n",
            "step 8900: train loss 2.4763, val loss 2.4997\n",
            "step 9000: train loss 2.4772, val loss 2.4993\n",
            "step 9100: train loss 2.4769, val loss 2.4927\n",
            "step 9200: train loss 2.4724, val loss 2.4935\n",
            "step 9300: train loss 2.4617, val loss 2.5031\n",
            "step 9400: train loss 2.4700, val loss 2.4953\n",
            "step 9500: train loss 2.4709, val loss 2.4956\n",
            "step 9600: train loss 2.4747, val loss 2.5095\n",
            "step 9700: train loss 2.4786, val loss 2.5003\n",
            "step 9800: train loss 2.4728, val loss 2.4978\n",
            "step 9900: train loss 2.4735, val loss 2.5010\n",
            "step 9999: train loss 2.4728, val loss 2.5006\n",
            "\n",
            "m\n",
            "RWARDWAbrartyomas f bexs\n",
            "QUKEMOfomithengneide, be; nglasey t SMapp w anced, t pe-g y I w t sls ghain\n",
            "Whe\n",
            "\n",
            "UD:\n",
            "baghe--thak On, outhalyo, k m tast y!\n",
            "itepl, it bupe, LAng irdis cther OPRED:\n",
            "\n",
            "y t ise ROudamy th.\n",
            "hewister'Tioouchet totandow, d l.\n",
            "HB: faghy,\n",
            "Hye V:\n",
            "GUKCOLSAw\n",
            "\n",
            "Theado? ce ge grslerityoungor llde p; s, tod As pld shyod pou\n",
            "nom, t gns th sthn fust nd h inonlldaist bt 'dit anom, t;\n",
            "S ha: incensus, pos.\n",
            "s.\n",
            "\n",
            "\n",
            "H:\n",
            "Thivyer s sploulleat st INont bo d acteathy'stago MBino'se, d ftont sIESAved s:\n",
            "Sam,\n",
            "H tthebosatthtowoucanit\n",
            "Lello lero, swararave'Tofo tu warve thasy quthag y l t trallio t aste ind t, ang go anve: my har?\n",
            "CLund tun he whimon heng\n",
            "Wh st y, Vateesous,\n",
            "INIUS:\n",
            "\n",
            "tant Oupt imoos thaby amas by nst pamaneret E:\n",
            "Maty:\n",
            "GLimyoul f\n",
            "mes; ersld oo fthixp'w INSe LA tcutrstunther pryou sprqupe g ty:\n",
            "I:\n",
            "s atht, th d:\n",
            "Tian f' thal. bowense heloven ou\n",
            "MBode INattll-def he, henofathoupare Mandolly h he n ane y s west t;\n",
            "LONRUShecadomiet:\n",
            "RIO:\n",
            "\n",
            "I nd, wn ie ll He m IUS Jearor s hathos the withey:\n",
            "CERUpoond horr porcontus is\n",
            "Loplloru, pequpe\n",
            "angaby Biighessanoucupe f,\n",
            "WAnt omu nt,\n",
            "olyort.\n",
            "\n",
            "IShape Ril seal agny shemesit Whe,\n",
            "Theruthame, IDI omite,\n",
            "msthavilllvittllithad,\n",
            "-lld n I wetit n:\n",
            "Sos dacof\n",
            "LEio me, t inssedam folt; o taya; bongstha JBuce\n",
            "\n",
            "Wirat mys:\n",
            "tilit atorvadigreord h toul o uthonges wrng, uch hy sod hourastome a'dalf lchafiletren stagou; a y wil fthayide?\n",
            "WCUSeigherlone batasar\n",
            "\n",
            "Ahe t ty y y t pl y shue quCE:\n",
            "ISet,\n",
            "A top neat deShes, yanth O, seanewispe e ts:\n",
            "Whas g\n",
            "\n",
            "a ly HAbugrvonnawenit;\n",
            "ct vet ol candond y agupedorit, K:\n",
            "Whinen ke, gle sen wiccans!\n",
            "bish cishARI drersothoorioud ce-I us dem CE, balam\n",
            "Dure\n",
            "\n",
            "\n",
            "myrysustheve haay\n",
            "\n",
            "Y, buo,\n",
            "IIUEI AROs: avaremase ice s.\n",
            "Mich beg, minou maiourmy: ayain d erdis dsse.\n",
            "\n",
            "\n",
            "And:\n",
            "Torou,\n",
            "Told ty the dolome, d datamee nat, heverd:\n",
            "Wwise, th ggyodisoman\n",
            "w fit talit, y that ild ancr qungghom garddeascan:\n",
            "Not I ly, s? at P eand ase Orn fr, d s, wee Engh me weishe thu troupirmee\n",
            "PH:\n",
            "d, pitinghat ay;ser, d RCetthet cak akit sthou\n",
            "W\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Read the text data\n",
        "with open(\"/tiny-shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Model definition\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8pocuRp2Dxf",
        "outputId": "e355aa28-065c-411f-893b-37c8aa38d773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5060\n",
            "step 300: train loss 2.4199, val loss 2.4337\n",
            "step 400: train loss 2.3500, val loss 2.3563\n",
            "step 500: train loss 2.2961, val loss 2.3126\n",
            "step 600: train loss 2.2408, val loss 2.2501\n",
            "step 700: train loss 2.2053, val loss 2.2187\n",
            "step 800: train loss 2.1636, val loss 2.1870\n",
            "step 900: train loss 2.1226, val loss 2.1483\n",
            "step 1000: train loss 2.1017, val loss 2.1283\n",
            "step 1100: train loss 2.0683, val loss 2.1174\n",
            "step 1200: train loss 2.0376, val loss 2.0798\n",
            "step 1300: train loss 2.0256, val loss 2.0645\n",
            "step 1400: train loss 1.9919, val loss 2.0362\n",
            "step 1500: train loss 1.9696, val loss 2.0304\n",
            "step 1600: train loss 1.9625, val loss 2.0470\n",
            "step 1700: train loss 1.9402, val loss 2.0119\n",
            "step 1800: train loss 1.9085, val loss 1.9957\n",
            "step 1900: train loss 1.9080, val loss 1.9869\n",
            "step 2000: train loss 1.8834, val loss 1.9941\n",
            "step 2100: train loss 1.8727, val loss 1.9758\n",
            "step 2200: train loss 1.8585, val loss 1.9622\n",
            "step 2300: train loss 1.8537, val loss 1.9503\n",
            "step 2400: train loss 1.8419, val loss 1.9424\n",
            "step 2500: train loss 1.8153, val loss 1.9407\n",
            "step 2600: train loss 1.8267, val loss 1.9374\n",
            "step 2700: train loss 1.8126, val loss 1.9344\n",
            "step 2800: train loss 1.8054, val loss 1.9230\n",
            "step 2900: train loss 1.8045, val loss 1.9339\n",
            "step 3000: train loss 1.7963, val loss 1.9243\n",
            "step 3100: train loss 1.7691, val loss 1.9208\n",
            "step 3200: train loss 1.7506, val loss 1.9092\n",
            "step 3300: train loss 1.7548, val loss 1.9038\n",
            "step 3400: train loss 1.7582, val loss 1.8960\n",
            "step 3500: train loss 1.7376, val loss 1.8934\n",
            "step 3600: train loss 1.7232, val loss 1.8888\n",
            "step 3700: train loss 1.7280, val loss 1.8814\n",
            "step 3800: train loss 1.7221, val loss 1.8951\n",
            "step 3900: train loss 1.7228, val loss 1.8789\n",
            "step 4000: train loss 1.7168, val loss 1.8635\n",
            "step 4100: train loss 1.7168, val loss 1.8798\n",
            "step 4200: train loss 1.7088, val loss 1.8672\n",
            "step 4300: train loss 1.6995, val loss 1.8501\n",
            "step 4400: train loss 1.7096, val loss 1.8686\n",
            "step 4500: train loss 1.6907, val loss 1.8546\n",
            "step 4600: train loss 1.6868, val loss 1.8348\n",
            "step 4700: train loss 1.6786, val loss 1.8346\n",
            "step 4800: train loss 1.6659, val loss 1.8445\n",
            "step 4900: train loss 1.6711, val loss 1.8384\n",
            "step 5000: train loss 1.6623, val loss 1.8231\n",
            "step 5100: train loss 1.6695, val loss 1.8326\n",
            "step 5200: train loss 1.6639, val loss 1.8206\n",
            "step 5300: train loss 1.6657, val loss 1.8215\n",
            "step 5400: train loss 1.6510, val loss 1.8186\n",
            "step 5500: train loss 1.6533, val loss 1.7999\n",
            "step 5600: train loss 1.6572, val loss 1.8126\n",
            "step 5700: train loss 1.6538, val loss 1.8137\n",
            "step 5800: train loss 1.6408, val loss 1.8069\n",
            "step 5900: train loss 1.6427, val loss 1.8025\n",
            "step 6000: train loss 1.6416, val loss 1.8054\n",
            "step 6100: train loss 1.6378, val loss 1.7866\n",
            "step 6200: train loss 1.6419, val loss 1.7910\n",
            "step 6300: train loss 1.6263, val loss 1.7916\n",
            "step 6400: train loss 1.6266, val loss 1.8014\n",
            "step 6500: train loss 1.6252, val loss 1.7859\n",
            "step 6600: train loss 1.6222, val loss 1.7820\n",
            "step 6700: train loss 1.6225, val loss 1.8044\n",
            "step 6800: train loss 1.6205, val loss 1.7999\n",
            "step 6900: train loss 1.6149, val loss 1.7875\n",
            "step 7000: train loss 1.6156, val loss 1.7827\n",
            "step 7100: train loss 1.6201, val loss 1.7854\n",
            "step 7200: train loss 1.6111, val loss 1.7919\n",
            "step 7300: train loss 1.6096, val loss 1.7709\n",
            "step 7400: train loss 1.6096, val loss 1.7861\n",
            "step 7500: train loss 1.5971, val loss 1.7893\n",
            "step 7600: train loss 1.6028, val loss 1.7803\n",
            "step 7700: train loss 1.5944, val loss 1.7718\n",
            "step 7800: train loss 1.6075, val loss 1.7672\n",
            "step 7900: train loss 1.6053, val loss 1.7707\n",
            "step 8000: train loss 1.5975, val loss 1.7717\n",
            "step 8100: train loss 1.6001, val loss 1.7770\n",
            "step 8200: train loss 1.6006, val loss 1.7825\n",
            "step 8300: train loss 1.5888, val loss 1.7603\n",
            "step 8400: train loss 1.5945, val loss 1.7733\n",
            "step 8500: train loss 1.5904, val loss 1.7711\n",
            "step 8600: train loss 1.5917, val loss 1.7695\n",
            "step 8700: train loss 1.5844, val loss 1.7701\n",
            "step 8800: train loss 1.5731, val loss 1.7677\n",
            "step 8900: train loss 1.5850, val loss 1.7448\n",
            "step 9000: train loss 1.5847, val loss 1.7643\n",
            "step 9100: train loss 1.5767, val loss 1.7637\n",
            "step 9200: train loss 1.5841, val loss 1.7616\n",
            "step 9300: train loss 1.5687, val loss 1.7457\n",
            "step 9400: train loss 1.5737, val loss 1.7483\n",
            "step 9500: train loss 1.5743, val loss 1.7633\n",
            "step 9600: train loss 1.5738, val loss 1.7561\n",
            "step 9700: train loss 1.5693, val loss 1.7401\n",
            "step 9800: train loss 1.5567, val loss 1.7577\n",
            "step 9900: train loss 1.5692, val loss 1.7480\n",
            "step 9999: train loss 1.5600, val loss 1.7477\n",
            "\n",
            "\n",
            "SOMENEN:\n",
            "Good lew thou kladied, will befor by the deconds,\n",
            "Till thee writhe oat ene\n",
            "Tright-sleap denings and wonk theshe canced chumbrances, my bake of bray:\n",
            "But, we deservance: banish'd dishour; whelf, scange so ho'd.\n",
            "\n",
            "VOMEN OF YORK:\n",
            "Rows will down since had I boyalted mornardance?\n",
            "Younging indeedle way: have no give,\n",
            "Alas we try hearts in use city.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Sir Eblack of her. Comeo, whither I fell, my like\n",
            "Call I pray tomatif my morralles' right?\n",
            "Of I well have tribe lew, propenish of that?\n",
            "\n",
            "CORIOLANUS:\n",
            "For Causes! what why, by leith, before to him one unstore. Hark, stay that doth curshiad.\n",
            "\n",
            "KING RICHARGE:\n",
            "Tull duke thee bod--thouse is tongued myself\n",
            "As and soverely insigning o' the\n",
            "insue minish ghand havy thembther.\n",
            "\n",
            "GALET:\n",
            "As all stake that nature as your poke!\n",
            "\n",
            "CORIOLANU:\n",
            "What is my kneel.\n",
            "\n",
            "Come:\n",
            "Metimest, and Roman your pray right, thee;\n",
            "My delive! die read; for yarefulled no;\n",
            "Which in from my meliest! what, and yet\n",
            "AUTHElS:\n",
            "Upon in that?\n",
            "Come. Yet, and is like it is wan youth only time to the will\n",
            "omplequirel with to play to young;\n",
            "Help now it sir, with you her, Stas it.\n",
            "\n",
            "CATESBY:\n",
            "NO, now that, and I all ne'ther'd of a mother,\n",
            "As Claint made one a.\n",
            "\n",
            "First:\n",
            "Lest, whither him, I have id this preace-ague;\n",
            "See the Plaracle and in for Geutle you;\n",
            "For I can I, 'tis a tight is have amed to ourgrate-give own did fishrike come.\n",
            "\n",
            "GLOUCESTER:\n",
            "For I tome:--O, thright is, that's live,\n",
            "And I and grieked yet: was yet, if I have a blass to a will,\n",
            "And stander-shall so wan no resillian:\n",
            "I will this your pusicybate.\n",
            "What, iffere's treason, had to us I had like her.\n",
            "\n",
            "GLOUCESTER:\n",
            "Marry ever up nase drums, and to tell that when sea meet courady,\n",
            "Thou by he\n",
            "is, ho; if I can: why did\n",
            "And but we ne'in come, so me, One assight'st blease.\n",
            "\n",
            "CORIOLANUS:\n",
            "Many her precious and loss with now, zengelo, and my fathers? bear, would free, and is them.\n",
            "\n",
            "CORIOLANUS:\n",
            "'Tis gook, and thy lament, sirup I see heel, to lady and for him\n",
            "to saish a by many finest by: but law, the duty her!\n",
            "\n",
            "JULIET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128  # Increased embedding size for potentially capturing more complex patterns\n",
        "n_head = 8  # Increased number of heads for better attention mechanism\n",
        "n_layer = 6  # Increased number of layers for deeper model\n",
        "dropout = 0.1  # Added dropout for regularization\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Read the text data\n",
        "with open(\"/tiny-shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Model definition\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHQwHofT6i4K",
        "outputId": "5af81020-ae34-4caf-9981-d32eab369053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.208385 M parameters\n",
            "step 0: train loss 4.3966, val loss 4.3933\n",
            "step 100: train loss 2.5379, val loss 2.5421\n",
            "step 200: train loss 2.4079, val loss 2.4178\n",
            "step 300: train loss 2.3163, val loss 2.3331\n",
            "step 400: train loss 2.2445, val loss 2.2557\n",
            "step 500: train loss 2.1731, val loss 2.2071\n",
            "step 600: train loss 2.1005, val loss 2.1513\n",
            "step 700: train loss 2.0653, val loss 2.1131\n",
            "step 800: train loss 2.0080, val loss 2.0741\n",
            "step 900: train loss 1.9803, val loss 2.0474\n",
            "step 1000: train loss 1.9356, val loss 2.0131\n",
            "step 1100: train loss 1.9146, val loss 2.0157\n",
            "step 1200: train loss 1.8889, val loss 1.9998\n",
            "step 1300: train loss 1.8611, val loss 1.9696\n",
            "step 1400: train loss 1.8248, val loss 1.9376\n",
            "step 1500: train loss 1.8100, val loss 1.9303\n",
            "step 1600: train loss 1.7866, val loss 1.9272\n",
            "step 1700: train loss 1.7806, val loss 1.9175\n",
            "step 1800: train loss 1.7717, val loss 1.9110\n",
            "step 1900: train loss 1.7556, val loss 1.8925\n",
            "step 2000: train loss 1.7417, val loss 1.8767\n",
            "step 2100: train loss 1.7267, val loss 1.8687\n",
            "step 2200: train loss 1.7085, val loss 1.8468\n",
            "step 2300: train loss 1.7090, val loss 1.8568\n",
            "step 2400: train loss 1.6991, val loss 1.8763\n",
            "step 2500: train loss 1.6851, val loss 1.8459\n",
            "step 2600: train loss 1.6988, val loss 1.8507\n",
            "step 2700: train loss 1.6833, val loss 1.8615\n",
            "step 2800: train loss 1.6665, val loss 1.8209\n",
            "step 2900: train loss 1.6605, val loss 1.8302\n",
            "step 3000: train loss 1.6569, val loss 1.8280\n",
            "step 3100: train loss 1.6410, val loss 1.8339\n",
            "step 3200: train loss 1.6407, val loss 1.8123\n",
            "step 3300: train loss 1.6304, val loss 1.8101\n",
            "step 3400: train loss 1.6310, val loss 1.7976\n",
            "step 3500: train loss 1.6361, val loss 1.8021\n",
            "step 3600: train loss 1.6301, val loss 1.7840\n",
            "step 3700: train loss 1.6076, val loss 1.7790\n",
            "step 3800: train loss 1.6088, val loss 1.7968\n",
            "step 3900: train loss 1.6055, val loss 1.7743\n",
            "step 4000: train loss 1.6103, val loss 1.7713\n",
            "step 4100: train loss 1.5995, val loss 1.7649\n",
            "step 4200: train loss 1.5889, val loss 1.7712\n",
            "step 4300: train loss 1.5769, val loss 1.7516\n",
            "step 4400: train loss 1.5860, val loss 1.7540\n",
            "step 4500: train loss 1.5709, val loss 1.7519\n",
            "step 4600: train loss 1.5782, val loss 1.7574\n",
            "step 4700: train loss 1.5667, val loss 1.7415\n",
            "step 4800: train loss 1.5775, val loss 1.7524\n",
            "step 4900: train loss 1.5651, val loss 1.7565\n",
            "step 5000: train loss 1.5597, val loss 1.7384\n",
            "step 5100: train loss 1.5584, val loss 1.7204\n",
            "step 5200: train loss 1.5578, val loss 1.7207\n",
            "step 5300: train loss 1.5575, val loss 1.7278\n",
            "step 5400: train loss 1.5442, val loss 1.7261\n",
            "step 5500: train loss 1.5426, val loss 1.7201\n",
            "step 5600: train loss 1.5522, val loss 1.7347\n",
            "step 5700: train loss 1.5251, val loss 1.7172\n",
            "step 5800: train loss 1.5433, val loss 1.7310\n",
            "step 5900: train loss 1.5433, val loss 1.7231\n",
            "step 6000: train loss 1.5379, val loss 1.7349\n",
            "step 6100: train loss 1.5330, val loss 1.6961\n",
            "step 6200: train loss 1.5314, val loss 1.7116\n",
            "step 6300: train loss 1.5233, val loss 1.7174\n",
            "step 6400: train loss 1.5240, val loss 1.7125\n",
            "step 6500: train loss 1.5218, val loss 1.6981\n",
            "step 6600: train loss 1.5276, val loss 1.7124\n",
            "step 6700: train loss 1.5113, val loss 1.7060\n",
            "step 6800: train loss 1.5167, val loss 1.7092\n",
            "step 6900: train loss 1.5178, val loss 1.7089\n",
            "step 7000: train loss 1.5003, val loss 1.7113\n",
            "step 7100: train loss 1.5189, val loss 1.6896\n",
            "step 7200: train loss 1.5059, val loss 1.6962\n",
            "step 7300: train loss 1.5052, val loss 1.6975\n",
            "step 7400: train loss 1.5102, val loss 1.6814\n",
            "step 7500: train loss 1.4972, val loss 1.6745\n",
            "step 7600: train loss 1.5029, val loss 1.6843\n",
            "step 7700: train loss 1.4998, val loss 1.6927\n",
            "step 7800: train loss 1.5023, val loss 1.6823\n",
            "step 7900: train loss 1.4980, val loss 1.6708\n",
            "step 8000: train loss 1.4894, val loss 1.6868\n",
            "step 8100: train loss 1.4953, val loss 1.6862\n",
            "step 8200: train loss 1.4854, val loss 1.6537\n",
            "step 8300: train loss 1.4937, val loss 1.6832\n",
            "step 8400: train loss 1.4717, val loss 1.6743\n",
            "step 8500: train loss 1.4811, val loss 1.6797\n",
            "step 8600: train loss 1.4924, val loss 1.6736\n",
            "step 8700: train loss 1.4936, val loss 1.6681\n",
            "step 8800: train loss 1.4840, val loss 1.6685\n",
            "step 8900: train loss 1.4812, val loss 1.6633\n",
            "step 9000: train loss 1.4808, val loss 1.6507\n",
            "step 9100: train loss 1.4795, val loss 1.6704\n",
            "step 9200: train loss 1.4785, val loss 1.6616\n",
            "step 9300: train loss 1.4686, val loss 1.6554\n",
            "step 9400: train loss 1.4705, val loss 1.6610\n",
            "step 9500: train loss 1.4692, val loss 1.6649\n",
            "step 9600: train loss 1.4685, val loss 1.6797\n",
            "step 9700: train loss 1.4711, val loss 1.6646\n",
            "step 9800: train loss 1.4688, val loss 1.6585\n",
            "step 9900: train loss 1.4739, val loss 1.6602\n",
            "step 9999: train loss 1.4628, val loss 1.6598\n",
            "\n",
            "So is a gods eye and have not begot.\n",
            "Peace that turn city this fourtune's habroped on thee\n",
            "Again: it but falled, my gales, but your\n",
            "have own by with them.\n",
            "\n",
            "RICHESS OV:\n",
            "An is they Dance, only nay, birst.\n",
            "\n",
            "LURTIO:\n",
            "Be knot my both one craftion.\n",
            "\n",
            "KING HENRY VI:\n",
            "What as mark a with me, crat own and cla tite execinity.\n",
            "\n",
            "EDWARD:\n",
            "Nay, before, father: I do; thought not the kill'st remoot, thy faurther shoiclit me\n",
            "A flious master all his beg are man\n",
            "Man-to carrkedied to my tongues, breath,\n",
            "Or it lie cannot the worled prucious put bid!\n",
            "\n",
            "GLOUCENTIES:\n",
            "Senatory Petecess!--\n",
            "\n",
            "DUKE OF YORK:\n",
            "Good no love of bloon! Play you most\n",
            "Would suchildWell tardsted time a drums asise he widow; maday your ropre fault,\n",
            "Haling ever his wiffe wear himself!\n",
            "\n",
            "GRERBY:\n",
            "Not clady these lord!'\n",
            "\n",
            "CATENBRUTER:\n",
            "This is the prisoners of my death.\n",
            "\n",
            "FRIAR LARIARp.\n",
            "\n",
            "QUEEN:\n",
            "My lord Margaries, King Henry Mamy be it not:\n",
            "But the look is the for matut virliame\n",
            "So for a thingo time of from us, thus this notiole--Heavens'\n",
            "'Tensy? On yield he thoughtst Mercust be content.\n",
            "He is go frett with me to mercy,\n",
            "But shout it. I take our care.'\n",
            "\n",
            "BUCKINGHAM:\n",
            "Why, we art go to the ends and Tars. How is thou here,\n",
            "Did hundfeet, in swain redeemen.\n",
            "\n",
            "KING RICHARD III:\n",
            "Edver it my from threee\n",
            "Looks seet follf wooes be to deshake a\n",
            "becorrest is uncounter for a king:\n",
            "At\n",
            "A bear thlerong is the ewoman Elbanquised so;\n",
            "That kind up some incortate, that spoitored so do.\n",
            "\n",
            "LORDY HEN:\n",
            "In Tyralling come, Duke miss, what any you sight him my heart and great thy boy\n",
            "In too enly'd to body, wood to Barkey?\n",
            "\n",
            "Poors:\n",
            "Nay, Hy drunk'd loquest, whence reconder to me,\n",
            "For have you will speak you are bry bount. Say, I say.\n",
            "\n",
            "Surm Citiveninio,\n",
            "Thut I leave the reliviner of him.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "A kissadow, I sabstand too't.\n",
            "\n",
            "HERBY:\n",
            "Year them? Pishop? I will him, cannot\n",
            "Thrie for Nor Murse! O, but int, to fing such a fair,\n",
            "Will be fry Redivil an this, thou appoint not conseit;\n",
            "That we forbid a reash in those armilmost paried me,\n",
            "And thy braintly weepings of b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DU9NLm2bBDSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}