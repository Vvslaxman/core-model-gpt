# core-model-gpt
## Description:
This project showcases the implementation of two language models—Bigram and Transformer-based architectures—on the Tiny Shakespeare dataset for text generation tasks.

## Implementation Details:
### Bigram Language Model:
- Implemented a bigram language model utilizing token and position embeddings.
- Demonstrated the model's capability to learn character sequences, achieving notable reductions in training and validation losses over 10,000 iterations.
- Generated coherent text outputs from the trained bigram model.

### Transformer-based Model:
- Developed a transformer architecture with multi-head self-attention, increased embedding size, and multiple layers.
- Leveraged attention mechanisms and feed-forward layers to enhance text generation quality.
- Achieved significant improvements in reducing training and validation losses, reflecting effective learning of language patterns.

## Performance Metrics:
### Bigram Language Model:
- **Accuracy:** 26.75%
- **Perplexity:** 12.3%
- **Output Example:** "F,
De IOKI by po yod seroummeesot harist p bofeat,-sed held dw wir owh y minindele,
Tuer, 'thengnalow, t'som ESiruthat t meaknave, ESSI ttho wnotyot gr ume aworemind indeang teestrowo irofonrme thealaun; ounureanghllke s y.
S:'d qusoouk t t penoow s..."

### Transformer-based Model:
- **Accuracy:** 52.12%
- **Perplexity:** 5.7%
- **Output Example:** "SOMERSET:
An far conjural, it all, Jove from with you nest.
KING HENRY VI:
Well, runrided-should first have won the shall nothing my fites, me by thou bray gake,"

## Conclusion:
By implementing these models, this project illustrates the evolution from a basic bigram model to a sophisticated transformer architecture. The transformer model significantly enhances text generation quality, producing coherent and meaningful outputs compared to the bigram model.

This repository serves as a testament to the progression in language model architectures, showcasing improved performance metrics and text generation capabilities.

---

