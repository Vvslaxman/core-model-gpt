# core-model-gpt
## Description:
This project involved implementing two language models—Bigram and Transformer-based architectures—on the Tiny Shakespeare dataset to demonstrate text generation capabilities.

## Implementation Details:
Bigram Language Model:

Developed a bigram language model that uses token and position embeddings.
Achieved training and validation loss reduction over 10,000 iterations, demonstrating the model's ability to learn character sequences.
Generated coherent text from the trained bigram model.
Transformer-based Model:

Implemented a transformer architecture with multiple attention heads, increased embedding size, and multiple layers.
Utilized multi-head self-attention and feed-forward layers for enhanced text generation.
The model significantly reduced training and validation losses, indicating effective learning of language patterns.
Output:

>Bigram Model Output:

Generated text with some coherence and correct character sequences.
Example output: "Y torer hin, je dlle s t ill chanelin m nehat ieyed CUK..."
>Transformer Model Output:

Produced more coherent and meaningful text compared to the bigram model.
Example output: "So is a gods eye and have not begot. Peace that turn city this fourtune's habroped..."
By implementing these models, I showcased the progression from a simple bigram model to a more sophisticated transformer architecture, highlighting the improvement in text generation quality.


